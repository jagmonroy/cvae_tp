# RUN PARAMETERS

model: 'vae'
test_list: ["eth", "hotel", "univ", "zara1", "zara2"]
id_list: ["0"]
prefix: "vae"
results_path: "results/results_ethucy"
data_path: "ETH-UCY/datasets"
# if there is validation and test set to 1. If there is only test, set to 0
formal_training: 1


# ABLATION HYPERPARAMETERS

use_teacherf: 0
use_gt_sampling: 1
use_features: 0 
  

# FEATURES HYPERPARAMETERS
    
# only pedestrians at most 20 m are considered in the APG
max_d: 20.0
# reduce have to divide 360. Number of "slices" (pizza) in APG
reduce: 360
    

# LSTM (ENCODER AND DECODER) AND LATENT SPACE HYPERPARAMETS

# before the LSTM, the data are transformed with a dense layer. "f_dis_D" is the dimension
f_dis_D: 128
# LSTM dimension
LSTM_D: 256
# latent space dimension
space_D: 128
           

# SAMPLING HYPERPARAMETERS

# number of predictions in validation 
outputs_validation: 20
# number of predictions in the final test
outputs_final_test: 100
    

# TRAINING HYPERPARAMETERS 

# bool: if it is 1 the models are trained with SGD, otherwise are trained with ADAM
sgd: 1
lr: 0.005
momentum: 0.9
decay: 0.0
# KL divergence loss term. In experiments it is 0.25
kl_w: 0.25
epochs: 100
transform_data: 0
# there are two different batch sizes. One is used to train and the other one to validate and test
batch_size: 128
batch_size_vali: 2048


# BETTER DO NOT CHANGE...

obs_l: 8
pre_l: 12
tot_l: 20
D: 2
decoders: 1
shuffle: 1